<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>第二篇：JDBC操作Hive</title>
    <url>/2020/05/29/hello-world/</url>
    <content><![CDATA[<h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><h2 id="确保如下环境已经完成安装部署："><a href="#确保如下环境已经完成安装部署：" class="headerlink" title="确保如下环境已经完成安装部署："></a>确保如下环境已经完成安装部署：</h2><ul>
<li>JDK的版本：1.8+</li>
<li>Hadoop版本：2.7.3</li>
<li>Hive版本：2.3.3</li>
<li>确保Hadoop环境都已经正常启动</li>
</ul>
<h2 id="启动hiveserver2服务"><a href="#启动hiveserver2服务" class="headerlink" title="启动hiveserver2服务"></a>启动hiveserver2服务</h2><p>说明：第三方如需连接hive，需要依赖hiveserver2服务，即依赖socket来进行通信，其模式是C/S模式。</p>
<ul>
<li>启动服务：以后台进程方式启动</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure>

<ul>
<li>检查服务是否正常启动：该服务默认运行在10000端口，只需检查10000端口是否处于监听状态</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">netstat -anop|grep 10000</span><br></pre></td></tr></table></figure>

<h2 id="在hive中创建mydb数据库"><a href="#在hive中创建mydb数据库" class="headerlink" title="在hive中创建mydb数据库"></a>在hive中创建mydb数据库</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create database mydb comment &#39;This is My DB&#39;;</span><br></pre></td></tr></table></figure>

<h2 id="在mydb数据库中创建emp表"><a href="#在mydb数据库中创建emp表" class="headerlink" title="在mydb数据库中创建emp表"></a>在mydb数据库中创建emp表</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create table if not exists emp(</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string,</span><br><span class="line">sal int,</span><br><span class="line">comm int,</span><br><span class="line">deptno int</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;,&#39;</span><br><span class="line">lines terminated by &#39;\n&#39;;</span><br></pre></td></tr></table></figure>

<h2 id="添加数据到emp表中"><a href="#添加数据到emp表中" class="headerlink" title="添加数据到emp表中"></a>添加数据到emp表中</h2><ul>
<li>数据源是emp.csv，已经上传到服务器，具体内容如下所示：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">7369,SMITH,CLERK,7902,1980&#x2F;12&#x2F;17,800,,20</span><br><span class="line">7499,ALLEN,SALESMAN,7698,1981&#x2F;2&#x2F;20,1600,300,30</span><br><span class="line">7521,WARD,SALESMAN,7698,1981&#x2F;2&#x2F;22,1250,500,30</span><br><span class="line">7566,JONES,MANAGER,7839,1981&#x2F;4&#x2F;2,2975,,20</span><br><span class="line">7654,MARTIN,SALESMAN,7698,1981&#x2F;9&#x2F;28,1250,1400,30</span><br><span class="line">7698,BLAKE,MANAGER,7839,1981&#x2F;5&#x2F;1,2850,,30</span><br><span class="line">7782,CLARK,MANAGER,7839,1981&#x2F;6&#x2F;9,2450,,10</span><br><span class="line">7788,SCOTT,ANALYST,7566,1987&#x2F;4&#x2F;19,3000,,20</span><br><span class="line">7839,KING,PRESIDENT,,1981&#x2F;11&#x2F;17,5000,,10</span><br><span class="line">7844,TURNER,SALESMAN,7698,1981&#x2F;9&#x2F;8,1500,0,30</span><br><span class="line">7876,ADAMS,CLERK,7788,1987&#x2F;5&#x2F;23,1100,,20</span><br><span class="line">7900,JAMES,CLERK,7698,1981&#x2F;12&#x2F;3,9500,,30</span><br><span class="line">7902,FORD,ANALYST,7566,1981&#x2F;12&#x2F;3,3000,,20</span><br><span class="line">7934,MILLER,CLERK,7782,1982&#x2F;1&#x2F;23,1300,,10</span><br></pre></td></tr></table></figure>
<ul>
<li>在hive cli中执行如下命令实现将数据从Linux本地文件系统中加载到emp表中</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">load data local inpath &#39;file:&#x2F;&#x2F;&#x2F;root&#x2F;emp.csv&#39; overwrite into table emp;</span><br></pre></td></tr></table></figure>

<h1 id="开发步骤"><a href="#开发步骤" class="headerlink" title="开发步骤"></a>开发步骤</h1><h2 id="创建工程：在IDEA中创建项目"><a href="#创建工程：在IDEA中创建项目" class="headerlink" title="创建工程：在IDEA中创建项目"></a>创建工程：在IDEA中创建项目</h2><h2 id="添加maven支持，并在pom-xml中添加依赖"><a href="#添加maven支持，并在pom-xml中添加依赖" class="headerlink" title="添加maven支持，并在pom.xml中添加依赖"></a>添加maven支持，并在pom.xml中添加依赖</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hive&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hive-jdbc&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.3.3&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>

<h2 id="编写代码"><a href="#编写代码" class="headerlink" title="编写代码"></a>编写代码</h2><ul>
<li>创建HiveDemo类</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import java.sql.*;</span><br><span class="line"></span><br><span class="line">public class HiveDemo &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;);</span><br><span class="line">        Connection connection &#x3D; DriverManager.getConnection(&quot;jdbc:hive2:&#x2F;&#x2F;192.168.215.132:10000&#x2F;mydb&quot;);</span><br><span class="line">        Statement statement &#x3D; connection.createStatement();</span><br><span class="line">        ResultSet resultSet &#x3D; statement.executeQuery(&quot;select deptno,sum((cast((comm is not null) as int) + sal)) sums from emp group by deptno order by sums desc&quot;);</span><br><span class="line">        while (resultSet.next()) &#123;</span><br><span class="line">            System.out.println(resultSet.getInt(1) + &quot;,&quot;</span><br><span class="line">                    + resultSet.getInt(2));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="运行测试"><a href="#运行测试" class="headerlink" title="运行测试"></a>运行测试</h2><ul>
<li>运行结果如下所示：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">30,17954</span><br><span class="line">20,10875</span><br><span class="line">10,8750</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>第一篇：Hadoop简介</title>
    <url>/2020/05/25/firstBlood/</url>
    <content><![CDATA[<h1 id="Hadoop基本概念"><a href="#Hadoop基本概念" class="headerlink" title="Hadoop基本概念"></a>Hadoop基本概念</h1><ul>
<li><p>Hadoop是什么</p>
<p>  Hadoop是一个由Apache基金会所开发的分布式系统基础架构,作者Doug Cutting受google Lab的开发的 Map/Reduce 和 Google File System(GFS) 的启发来设计，hadoop核心架构是MapReduce编程模型和HDFS分布式文件系统。  </p>
</li>
<li><p>HDFS是什么<br>  HDFS（ Hadoop Distributed File System）分布式文件系统，为海量数据提供存储服务，将大文件（大于64M/128M）拆分为块（每个块64M或者128M）,多节点存放。具有高吞吐量、高容错性。</p>
</li>
<li><p>MapReduce是什么<br>  MapReduce框架可以把一个应用程序分解为许多并行计算指令，跨大量的计算节点运行非常巨大的数据集。采用“分而治之”的思想，Map用来切分大的数据，Reduce用来合并Map计算的结果。</p>
</li>
</ul>
<h2 id="Hadoop的起源"><a href="#Hadoop的起源" class="headerlink" title="Hadoop的起源"></a>Hadoop的起源</h2><ul>
<li><p>思想来源：Google的三篇论文<br>  a GFS：Google File System<br>  b MapReduce : 分布式计算模型<br>  c BigTable : 构建与HDFS之上的面向列的NoSQL数据库</p>
</li>
<li><p>发展过程<br>  2003-2004年，Google公开了部分GFS和Mapreduce思想的细节，以此为基础DougCutting等人用了2年业余时间实现了DFS和Mapreduce机制，使Nutch性能飙升Yahoo招安Doug Cutting及其项目Hadoop 于 2005 年秋天作为 Lucene的子项目Nutch的 一部分正式引入Apache基金会。2006 年 3 月份，Map-Reduce 和 Nutch Distributed File System (NDFS) 分别被纳入称为Hadoop 的项目中。</p>
</li>
</ul>
<h2 id="Hadoop的生态圈"><a href="#Hadoop的生态圈" class="headerlink" title="Hadoop的生态圈"></a>Hadoop的生态圈</h2><p>Hadoop生态圈包含很多开源的组件，比如HBase、Hive、Pig、OOize、Zookeeper、Flume等，具体如下图所示：<br><img src="/images/hadoop.png" alt="Hadoop生态圈"> </p>
<ul>
<li><p>HBase简介<br>  HBASE是apahce的开源KV型数据库，是建立的hdfs之上，提供高可靠性、高性能、列存储、可伸缩、实时读写的数据库系统。基于google发表的bigTable论文的灵感设计出来的。它介于nosql和RDBMS之间，仅能通过主键(row key)和主键的range来检索数据，仅支持单行事务(可通过hive支持来实现多表join等复杂操作)。主要用来存储非结构化和半结构化的松散数据。与hadoop一样，Hbase目标主要依靠横向扩展，通过不断增加廉价的商用服务器，来增加计算和存储能力。</p>
</li>
<li><p>zookeeper简介<br>  zookeeper是一个高性能，分布式的，开源分布式应用协调服务，是storm、hbase的重要组件，它是一个为分布式应用提供一致性服务的软件。它提供了简单原始的功能，分布式应用可以基于它实现更高级的服务，比如同步，配置管理，集群管理，名空间。它被设计为易于编程，使用文件系统目录树作为数据模型。服务端跑在java上，提供java和C的客户端API。Zookeeper服务自身组成一个集群(2n+1个服务允许n个失效)。Zookeeper服务有两个角色，一个是leader，负责写服务和数据同步，剩下的是follower，提供读服务，leader失效后会在follower中重新选举新的leader。</p>
</li>
</ul>
<h2 id="Hadoop的历史版本"><a href="#Hadoop的历史版本" class="headerlink" title="Hadoop的历史版本"></a>Hadoop的历史版本</h2><ul>
<li>0.20.x系列</li>
<li>0.21.0/0.22.x系列</li>
<li>0.23.X系列</li>
<li>2.X系列</li>
<li>Hadoop版本发展脉络图<br><img src="/images/hadoop_history.png" alt="Hadoop历史发展脉络图"></li>
</ul>
<h2 id="Hadoop的组成"><a href="#Hadoop的组成" class="headerlink" title="Hadoop的组成"></a>Hadoop的组成</h2><ul>
<li>common</li>
<li>hdfs</li>
<li>MapReduce</li>
<li>yarn</li>
</ul>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>使用HiveQL分析电商日志</title>
    <url>/2020/06/05/%E4%BD%BF%E7%94%A8HiveQL%E5%88%86%E6%9E%90%E7%94%B5%E5%95%86%E6%97%A5%E5%BF%97/</url>
    <content><![CDATA[<h1 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h1><p>利用HiveQL实现对电商日志的分析，求出topN商品的信息。</p>
<h1 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h1><p>学会使用HiveQL对电商日志进行清洗和分析，用熟悉的SQL语句替代编写MapReduce程序。</p>
<h1 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h1><p>本文所使用的环境如下：</p>
<ul>
<li>CentOS7</li>
<li>Jdk-1.8.+</li>
<li>Hadoop-2.7.3</li>
<li>Hive-2.3.3</li>
</ul>
<h1 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h1><p>完成该案例演示所需资源如下：</p>
<ul>
<li><p>测试数据源</p>
<p>  文件名称：access.log<br>  文件具体内容如下：</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">niit110,192.168.215.131 - - [28&#x2F;May&#x2F;2019:18:11:44 +0800] &quot;GET &#x2F;shop&#x2F;detail.html?id&#x3D;402857036a2831e001kshdksdsdk89912 HTTP&#x2F;1.0&quot; 200 4391 &quot;-&quot; &quot;ApacheBench&#x2F;2.3&quot; &quot;-&quot;</span><br><span class="line">niit110,192.168.215.131 - - [28&#x2F;May&#x2F;2019:18:11:44 +0800] &quot;GET &#x2F;shop&#x2F;detail.html?id&#x3D;402857036a2831e001kshdksdsdk89923 HTTP&#x2F;1.0&quot; 200 4391 &quot;-&quot; &quot;ApacheBench&#x2F;2.3&quot; &quot;-&quot;</span><br><span class="line">niit110,192.168.215.131 - - [28&#x2F;May&#x2F;2019:18:11:44 +0800] &quot;GET &#x2F;shop&#x2F;detail.html?id&#x3D;402857036a2831e001kshdksdsdk89933 HTTP&#x2F;1.0&quot; 200 4391 &quot;-&quot; &quot;ApacheBench&#x2F;2.3&quot; &quot;-&quot;</span><br><span class="line">niit110,192.168.215.131 - - [28&#x2F;May&#x2F;2019:18:11:44 +0800] &quot;GET &#x2F;shop&#x2F;detail.html?id&#x3D;402857036a2831e001kshdksdsdk89933 HTTP&#x2F;1.0&quot; 200 4391 &quot;-&quot; &quot;ApacheBench&#x2F;2.3&quot; &quot;-&quot;</span><br><span class="line">niit110,192.168.215.131 - - [28&#x2F;May&#x2F;2019:18:11:44 +0800] &quot;GET &#x2F;shop&#x2F;detail.html?id&#x3D;402857036a2831e001kshdksdsdk89944 HTTP&#x2F;1.0&quot; 200 4391 &quot;-&quot; &quot;ApacheBench&#x2F;2.3&quot; &quot;-&quot;</span><br><span class="line">niit110,192.168.215.131 - - [28&#x2F;May&#x2F;2019:18:11:44 +0800] &quot;GET &#x2F;shop&#x2F;detail.html?id&#x3D;402857036a2831e001kshdksdsdk89944 HTTP&#x2F;1.0&quot; 200 4391 &quot;-&quot; &quot;ApacheBench&#x2F;2.3&quot; &quot;-&quot;</span><br><span class="line">niit110,192.168.215.131 - - [28&#x2F;May&#x2F;2019:18:11:44 +0800] &quot;GET &#x2F;shop&#x2F;detail.html?id&#x3D;402857036a2831e001kshdksdsdk89912 HTTP&#x2F;1.0&quot; 200 4391 &quot;-&quot; &quot;ApacheBench&#x2F;2.3&quot; &quot;-&quot;</span><br><span class="line">niit110,192.168.215.131 - - [28&#x2F;May&#x2F;2019:18:11:44 +0800] &quot;GET &#x2F;shop&#x2F;detail.html?id&#x3D;402857036a2831e001kshdksdsdk89923 HTTP&#x2F;1.0&quot; 200 4391 &quot;-&quot; &quot;ApacheBench&#x2F;2.3&quot; &quot;-&quot;</span><br><span class="line">niit110,192.168.215.131 - - [28&#x2F;May&#x2F;2019:18:11:44 +0800] &quot;GET &#x2F;shop&#x2F;detail.html?id&#x3D;402857036a2831e001kshdksdsdk89933 HTTP&#x2F;1.0&quot; 200 4391 &quot;-&quot; &quot;ApacheBench&#x2F;2.3&quot; &quot;-&quot;</span><br><span class="line">niit110,192.168.215.131 - - [28&#x2F;May&#x2F;2019:18:11:44 +0800] &quot;GET &#x2F;shop&#x2F;detail.html?id&#x3D;402857036a2831e001kshdksdsdk89933 HTTP&#x2F;1.0&quot; 200 4391 &quot;-&quot; &quot;ApacheBench&#x2F;2.3&quot; &quot;-&quot;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>连接工具<br>  putty</li>
</ul>
<h1 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h1><ul>
<li><p>分析日志</p>
<p>  思路：上述资源中所列出的日志信息，其中/shop/detail.html?id=402857036a2831e001kshdksdsdk89923代表的是id为    402857036a2831e001kshdksdsdk89923的商品的URI，我们只需要找出所有的的商品URI，对其进行分组统计，就可以得出每种商品被访问过的次数，进而可以对其访问量进行排名，实现topN的功能。</p>
<p>  方法：经过观察，我们发现每行访问记录中大部分都是以空格形式进行间隔的，而且目标URI两端都是被空格分开的，由此我们可以按照空格对每行文本进行分割，进而选择空格作为下一步创建外部表的列的分隔符。</p>
</li>
<li><p>上传日志文件到服务器</p>
<p>  将access.log上传到服务器上的HDFS下的/datas/logs目录下</p>
</li>
<li><p>建立hive外部表</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create external table logs_ex(client_ip string,crossbar1 string,crossbar2 string,time string,time_sufix string,req string,uri string,protocol string,status int,count int,crossbar3 string,user_agent string,crossbar4 string) row format delimited fields terminated by &#39; &#39; lines terminated by &#39;\n&#39; location &#39;&#x2F;datas&#x2F;logs&#39;;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写分析的SQL语句</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select uri,count(1) total from logs_ex group by uri order by total desc limit 3;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看结果</p>
</li>
</ul>
<p><img src="/images/result.png" alt="结果"></p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><pre><code>本文使用HiveQL简单实现了对电商日志的分析，其关键点在于了解如何分析日志的思路和方法，找到我们所需要的目标字段，然后根据日志的格式创建Hive外部表，最后通过编写SQL语句来实现。本文仅作为学习HiveQL来实现某一个应该场景而已。实际生产工作中，还要具体问题具体分析。祝各位读者道友们一切顺利~</code></pre>]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
</search>
