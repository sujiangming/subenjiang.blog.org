<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>第二篇：JDBC操作Hive</title>
    <url>/2020/05/29/hello-world/</url>
    <content><![CDATA[<h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><h2 id="确保如下环境已经完成安装部署："><a href="#确保如下环境已经完成安装部署：" class="headerlink" title="确保如下环境已经完成安装部署："></a>确保如下环境已经完成安装部署：</h2><ul>
<li>JDK的版本：1.8+</li>
<li>Hadoop版本：2.7.3</li>
<li>Hive版本：2.3.3</li>
<li>确保Hadoop环境都已经正常启动</li>
</ul>
<h2 id="启动hiveserver2服务"><a href="#启动hiveserver2服务" class="headerlink" title="启动hiveserver2服务"></a>启动hiveserver2服务</h2><p>说明：第三方如需连接hive，需要依赖hiveserver2服务，即依赖socket来进行通信，其模式是C/S模式。</p>
<ul>
<li>启动服务：以后台进程方式启动</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure>

<ul>
<li>检查服务是否正常启动：该服务默认运行在10000端口，只需检查10000端口是否处于监听状态</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">netstat -anop|grep 10000</span><br></pre></td></tr></table></figure>

<h2 id="在hive中创建mydb数据库"><a href="#在hive中创建mydb数据库" class="headerlink" title="在hive中创建mydb数据库"></a>在hive中创建mydb数据库</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create database mydb comment &#39;This is My DB&#39;;</span><br></pre></td></tr></table></figure>

<h2 id="在mydb数据库中创建emp表"><a href="#在mydb数据库中创建emp表" class="headerlink" title="在mydb数据库中创建emp表"></a>在mydb数据库中创建emp表</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create table if not exists emp(</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string,</span><br><span class="line">sal int,</span><br><span class="line">comm int,</span><br><span class="line">deptno int</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;,&#39;</span><br><span class="line">lines terminated by &#39;\n&#39;;</span><br></pre></td></tr></table></figure>

<h2 id="添加数据到emp表中"><a href="#添加数据到emp表中" class="headerlink" title="添加数据到emp表中"></a>添加数据到emp表中</h2><ul>
<li>数据源是emp.csv，已经上传到服务器，具体内容如下所示：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">7369,SMITH,CLERK,7902,1980&#x2F;12&#x2F;17,800,,20</span><br><span class="line">7499,ALLEN,SALESMAN,7698,1981&#x2F;2&#x2F;20,1600,300,30</span><br><span class="line">7521,WARD,SALESMAN,7698,1981&#x2F;2&#x2F;22,1250,500,30</span><br><span class="line">7566,JONES,MANAGER,7839,1981&#x2F;4&#x2F;2,2975,,20</span><br><span class="line">7654,MARTIN,SALESMAN,7698,1981&#x2F;9&#x2F;28,1250,1400,30</span><br><span class="line">7698,BLAKE,MANAGER,7839,1981&#x2F;5&#x2F;1,2850,,30</span><br><span class="line">7782,CLARK,MANAGER,7839,1981&#x2F;6&#x2F;9,2450,,10</span><br><span class="line">7788,SCOTT,ANALYST,7566,1987&#x2F;4&#x2F;19,3000,,20</span><br><span class="line">7839,KING,PRESIDENT,,1981&#x2F;11&#x2F;17,5000,,10</span><br><span class="line">7844,TURNER,SALESMAN,7698,1981&#x2F;9&#x2F;8,1500,0,30</span><br><span class="line">7876,ADAMS,CLERK,7788,1987&#x2F;5&#x2F;23,1100,,20</span><br><span class="line">7900,JAMES,CLERK,7698,1981&#x2F;12&#x2F;3,9500,,30</span><br><span class="line">7902,FORD,ANALYST,7566,1981&#x2F;12&#x2F;3,3000,,20</span><br><span class="line">7934,MILLER,CLERK,7782,1982&#x2F;1&#x2F;23,1300,,10</span><br></pre></td></tr></table></figure>
<ul>
<li>在hive cli中执行如下命令实现将数据从Linux本地文件系统中加载到emp表中</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">load data local inpath &#39;file:&#x2F;&#x2F;&#x2F;root&#x2F;emp.csv&#39; overwrite into table emp;</span><br></pre></td></tr></table></figure>

<h1 id="开发步骤"><a href="#开发步骤" class="headerlink" title="开发步骤"></a>开发步骤</h1><h2 id="创建工程：在IDEA中创建项目"><a href="#创建工程：在IDEA中创建项目" class="headerlink" title="创建工程：在IDEA中创建项目"></a>创建工程：在IDEA中创建项目</h2><h2 id="添加maven支持，并在pom-xml中添加依赖"><a href="#添加maven支持，并在pom-xml中添加依赖" class="headerlink" title="添加maven支持，并在pom.xml中添加依赖"></a>添加maven支持，并在pom.xml中添加依赖</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hive&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hive-jdbc&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.3.3&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>

<h2 id="编写代码"><a href="#编写代码" class="headerlink" title="编写代码"></a>编写代码</h2><ul>
<li>创建HiveDemo类</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import java.sql.*;</span><br><span class="line"></span><br><span class="line">public class HiveDemo &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;);</span><br><span class="line">        Connection connection &#x3D; DriverManager.getConnection(&quot;jdbc:hive2:&#x2F;&#x2F;192.168.215.132:10000&#x2F;mydb&quot;);</span><br><span class="line">        Statement statement &#x3D; connection.createStatement();</span><br><span class="line">        ResultSet resultSet &#x3D; statement.executeQuery(&quot;select deptno,sum((cast((comm is not null) as int) + sal)) sums from emp group by deptno order by sums desc&quot;);</span><br><span class="line">        while (resultSet.next()) &#123;</span><br><span class="line">            System.out.println(resultSet.getInt(1) + &quot;,&quot;</span><br><span class="line">                    + resultSet.getInt(2));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="运行测试"><a href="#运行测试" class="headerlink" title="运行测试"></a>运行测试</h2><ul>
<li>运行结果如下所示：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">30,17954</span><br><span class="line">20,10875</span><br><span class="line">10,8750</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>第一篇：Hadoop简介</title>
    <url>/2020/05/25/firstBlood/</url>
    <content><![CDATA[<h1 id="Hadoop基本概念"><a href="#Hadoop基本概念" class="headerlink" title="Hadoop基本概念"></a>Hadoop基本概念</h1><ul>
<li><p>Hadoop是什么</p>
<p>  Hadoop是一个由Apache基金会所开发的分布式系统基础架构,作者Doug Cutting受google Lab的开发的 Map/Reduce 和 Google File System(GFS) 的启发来设计，hadoop核心架构是MapReduce编程模型和HDFS分布式文件系统。  </p>
</li>
<li><p>HDFS是什么<br>  HDFS（ Hadoop Distributed File System）分布式文件系统，为海量数据提供存储服务，将大文件（大于64M/128M）拆分为块（每个块64M或者128M）,多节点存放。具有高吞吐量、高容错性。</p>
</li>
<li><p>MapReduce是什么<br>  MapReduce框架可以把一个应用程序分解为许多并行计算指令，跨大量的计算节点运行非常巨大的数据集。采用“分而治之”的思想，Map用来切分大的数据，Reduce用来合并Map计算的结果。</p>
</li>
</ul>
<h2 id="Hadoop的起源"><a href="#Hadoop的起源" class="headerlink" title="Hadoop的起源"></a>Hadoop的起源</h2><ul>
<li><p>思想来源：Google的三篇论文<br>  a GFS：Google File System<br>  b MapReduce : 分布式计算模型<br>  c BigTable : 构建与HDFS之上的面向列的NoSQL数据库</p>
</li>
<li><p>发展过程<br>  2003-2004年，Google公开了部分GFS和Mapreduce思想的细节，以此为基础DougCutting等人用了2年业余时间实现了DFS和Mapreduce机制，使Nutch性能飙升Yahoo招安Doug Cutting及其项目Hadoop 于 2005 年秋天作为 Lucene的子项目Nutch的 一部分正式引入Apache基金会。2006 年 3 月份，Map-Reduce 和 Nutch Distributed File System (NDFS) 分别被纳入称为Hadoop 的项目中。</p>
</li>
</ul>
<h2 id="Hadoop的生态圈"><a href="#Hadoop的生态圈" class="headerlink" title="Hadoop的生态圈"></a>Hadoop的生态圈</h2><p>Hadoop生态圈包含很多开源的组件，比如HBase、Hive、Pig、OOize、Zookeeper、Flume等，具体如下图所示：<br><img src="/images/hadoop.png" alt="Hadoop生态圈"> </p>
<ul>
<li><p>HBase简介<br>  HBASE是apahce的开源KV型数据库，是建立的hdfs之上，提供高可靠性、高性能、列存储、可伸缩、实时读写的数据库系统。基于google发表的bigTable论文的灵感设计出来的。它介于nosql和RDBMS之间，仅能通过主键(row key)和主键的range来检索数据，仅支持单行事务(可通过hive支持来实现多表join等复杂操作)。主要用来存储非结构化和半结构化的松散数据。与hadoop一样，Hbase目标主要依靠横向扩展，通过不断增加廉价的商用服务器，来增加计算和存储能力。</p>
</li>
<li><p>zookeeper简介<br>  zookeeper是一个高性能，分布式的，开源分布式应用协调服务，是storm、hbase的重要组件，它是一个为分布式应用提供一致性服务的软件。它提供了简单原始的功能，分布式应用可以基于它实现更高级的服务，比如同步，配置管理，集群管理，名空间。它被设计为易于编程，使用文件系统目录树作为数据模型。服务端跑在java上，提供java和C的客户端API。Zookeeper服务自身组成一个集群(2n+1个服务允许n个失效)。Zookeeper服务有两个角色，一个是leader，负责写服务和数据同步，剩下的是follower，提供读服务，leader失效后会在follower中重新选举新的leader。</p>
</li>
</ul>
<h2 id="Hadoop的历史版本"><a href="#Hadoop的历史版本" class="headerlink" title="Hadoop的历史版本"></a>Hadoop的历史版本</h2><ul>
<li>0.20.x系列</li>
<li>0.21.0/0.22.x系列</li>
<li>0.23.X系列</li>
<li>2.X系列</li>
<li>Hadoop版本发展脉络图<br><img src="/images/hadoop_history.png" alt="Hadoop历史发展脉络图"></li>
</ul>
<h2 id="Hadoop的组成"><a href="#Hadoop的组成" class="headerlink" title="Hadoop的组成"></a>Hadoop的组成</h2><ul>
<li>common</li>
<li>hdfs</li>
<li>MapReduce</li>
<li>yarn</li>
</ul>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
</search>
